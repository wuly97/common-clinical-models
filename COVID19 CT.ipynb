{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“COVID-Train&Validation.ipynb”的副本",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wuly97/common-clinical-models/blob/master/COVID19%20CT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JmhSAzXPUaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7009fed4-5311-40c3-e781-7c7f2eda7d71"
      },
      "source": [
        "!nvidia-smi #show the allocated GPU"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Dec 14 07:49:13 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yqhPi-FS60r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c6c8280-d022-4e6f-96ed-66aa056e9598"
      },
      "source": [
        "#Connect your Google Drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UAp6XlqPeou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76cf8922-ce29-484b-8136-c780c065c58f"
      },
      "source": [
        "#Install essential libraries\n",
        "!pip install zipfile36\n",
        "!pip install git+https://github.com/mr7495/RetinaNet\n",
        "#Downgrade Tensorflow and Keras because updated versions may cause some errors\n",
        "#!pip uninstall tensorflow\n",
        "#!pip uninstall keras\n",
        "#!pip install tensorflow==2.2\n",
        "#!pip install keras==2.3.0\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting zipfile36\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/8a/3b7da0b0bd87d1ef05b74207827c72d348b56a0d6d83242582be18a81e02/zipfile36-0.1.3-py3-none-any.whl\n",
            "Installing collected packages: zipfile36\n",
            "Successfully installed zipfile36-0.1.3\n",
            "Collecting git+https://github.com/mr7495/RetinaNet\n",
            "  Cloning https://github.com/mr7495/RetinaNet to /tmp/pip-req-build-b2vrx7y0\n",
            "  Running command git clone -q https://github.com/mr7495/RetinaNet /tmp/pip-req-build-b2vrx7y0\n",
            "  Running command git submodule update --init --recursive -q\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (2.4.3)\n",
            "Collecting keras-resnet==0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/05/46/ad0b2d1a05d9497bd80c98a2c3f4d8be38a4601ace69af72814f5fafd851/keras-resnet-0.1.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (1.4.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (0.29.21)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (7.0.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (4.1.2.30)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from keras-retinanet==0.5.1) (3.38.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet==0.5.1) (2.10.0)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->keras-retinanet==0.5.1) (2.4.0)\n",
            "Building wheels for collected packages: keras-retinanet, keras-resnet\n",
            "  Building wheel for keras-retinanet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-retinanet: filename=keras_retinanet-0.5.1-cp36-cp36m-linux_x86_64.whl size=181969 sha256=e0ea72cef7cb3cf0e691fa4fdf64efa729d0b7abf16a9794655eb75e7e819b77\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gcg7x0tg/wheels/08/6d/f5/38473102b36a5975e02e8f339fbf85bc4d1b1c7c80dc68a595\n",
            "  Building wheel for keras-resnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-resnet: filename=keras_resnet-0.1.0-py2.py3-none-any.whl size=13343 sha256=30b8709bca153160320814e73b163aafdee88033e63b6ca803c4ac36f01f7488\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/dd/ac/842235b63dddac12faa4b48ebe58b8944e8c2e57c2e38dddb6\n",
            "Successfully built keras-retinanet keras-resnet\n",
            "Installing collected packages: keras-resnet, keras-retinanet\n",
            "Successfully installed keras-resnet-0.1.0 keras-retinanet-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LEwTHE7Zibj",
        "outputId": "b798eabe-ca85-4d98-ccbd-2eda5e440226"
      },
      "source": [
        "#Downgrade Tensorflow and Keras because updated versions may cause some errors\n",
        "!pip uninstall keras\n",
        "!pip install keras==2.3.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling Keras-2.4.3:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/Keras-2.4.3.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/docs/*\n",
            "    /usr/local/lib/python3.6/dist-packages/keras/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.6/dist-packages/docs/md_autogen.py\n",
            "    /usr/local/lib/python3.6/dist-packages/docs/update_docs.py\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled Keras-2.4.3\n",
            "Collecting keras==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/18/2e1ef121e5560ac24c7ac9e363aa5fa7006c40563c989e7211aba95b793a/Keras-2.3.0-py2.py3-none-any.whl (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 12.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (1.4.1)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0) (3.13)\n",
            "Installing collected packages: keras-applications, keras\n",
            "Successfully installed keras-2.3.0 keras-applications-1.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu_Ni5o_uLW8",
        "outputId": "4e198587-f92b-4f42-983b-e114a1599e44"
      },
      "source": [
        "!pip uninstall tensorflow\r\n",
        "!pip install tensorflow==2.2.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.3.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.3.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.3.0\n",
            "Collecting tensorflow==2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/be/679ce5254a8c8d07470efb4a4c00345fae91f766e64f1c2aece8796d7218/tensorflow-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2MB 32kB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (3.12.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.10.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.3.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.36.1)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.4.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.34.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.18.5)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/f5/926ae53d6a226ec0fda5208e0e581cffed895ccc89e36ba76a8e60895b78/tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 86kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (3.3.0)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/74/0a6fcb206dcc72a6da9a62dd81784bfdbff5fedb099982861dc2219014fb/tensorboard-2.2.2-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 44.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (2.10.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow==2.2.0) (50.3.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.3.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.17.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "Successfully installed tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw0BhQ7nPihU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "739e95a7-10d2-4e44-f7ca-700f241de13b"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import csv\n",
        "import zipfile\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Dropout, Flatten, Dense,Input\n",
        "from keras.applications.resnet_v2 import ResNet50V2\n",
        "from keras.applications.xception import Xception\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.initializers import RandomNormal\n",
        "import keras.backend as k\n",
        "from sklearn.utils import shuffle\n",
        "import io\n",
        "from PIL import Image as pil_image\n",
        "from keras_retinanet import layers\n",
        "import keras.backend as k\n",
        "import keras_retinanet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCUZApPBTyF0"
      },
      "source": [
        "# Add the COVID-CTset to your drive through this link:\n",
        "#https://drive.google.com/drive/folders/1xdk-mCkxCDNwsMAk2SGv203rY1mrbnPB?usp=sharing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBvmN8vkfP8u"
      },
      "source": [
        "#import shutil\r\n",
        "#shutil.rmtree('./data/Train&Validation')\r\n",
        "#os.mkdir('要清空的文件夹名')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhzh5DCPgYhQ"
      },
      "source": [
        "archive = zipfile.ZipFile('drive/My Drive/COVID-CTset/Train&Validation.zip') #Path to the shared data for training and validation\n",
        "for file in archive.namelist():\n",
        "     archive.extract(file, './data') #Extract the data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzN_Dxg5V14k"
      },
      "source": [
        "fold_num=5 #Select Fold Number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpjAiyH3bKIu"
      },
      "source": [
        "#Here we set the data generators for applying data augmentation methods\n",
        "train_datagen = ImageDataGenerator(horizontal_flip=True,vertical_flip=True,zoom_range=0.05,rotation_range=360,width_shift_range=0.05,height_shift_range=0.05,shear_range=0.05)\n",
        "test_datagen = ImageDataGenerator()\n",
        "train_df =pd.read_csv('drive/My Drive/COVID-CTset/CSV/train{}.csv'.format(fold_num)) #raed train csv file\n",
        "validation_df = pd.read_csv('drive/My Drive/COVID-CTset/CSV/validation{}.csv'.format(fold_num)) #raed validation csv file (Validation in the training process)\n",
        "train_df = shuffle(train_df) #Shuffle the train data\n",
        "test_df = pd.read_csv('drive/My Drive/COVID-CTset/CSV/test{}.csv'.format(fold_num))#raed test csv file (For evaluating the final version of the trained network)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ONkNg4I4Vju",
        "outputId": "32c96e03-7593-400e-da3f-889c3d204438"
      },
      "source": [
        "print (validation_df.head(100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                filename   class\n",
            "0    137covid_patient69_SR_3_IM00007.tif   covid\n",
            "1   157covid_patient141_SR_3_IM00017.tif   covid\n",
            "2    137covid_patient69_SR_2_IM00009.tif   covid\n",
            "3   157covid_patient141_SR_2_IM00027.tif   covid\n",
            "4   157covid_patient144_SR_2_IM00010.tif   covid\n",
            "..                                   ...     ...\n",
            "95   normal1_patient177_SR_2_IM00045.tif  normal\n",
            "96   normal2_patient280_SR_2_IM00020.tif  normal\n",
            "97   normal1_patient235_SR_2_IM00018.tif  normal\n",
            "98   normal1_patient240_SR_2_IM00023.tif  normal\n",
            "99   normal1_patient225_SR_2_IM00015.tif  normal\n",
            "\n",
            "[100 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bITv5T0wVT-f"
      },
      "source": [
        "shape=(512,512,1) #shape of the dataset images (in TIFF format)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvqnEK7hRf4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "867aedc4-7872-48f6-caf7-8071a9086b6c"
      },
      "source": [
        "#Create the generators\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "      dataframe=train_df,\n",
        "      directory='data',\n",
        "      x_col=\"filename\",\n",
        "      y_col=\"class\",\n",
        "      target_size=shape[:2],\n",
        "      batch_size=14,\n",
        "      validate_filenames=False,\n",
        "      class_mode='categorical',color_mode=\"grayscale\",shuffle=True)\n",
        "validation_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe=validation_df,\n",
        "        directory='data',\n",
        "        x_col=\"filename\",\n",
        "        y_col=\"class\",\n",
        "        target_size=shape[:2],\n",
        "        batch_size=10,\n",
        "        validate_filenames=False,\n",
        "        class_mode='categorical',color_mode=\"grayscale\",shuffle=True)\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe=test_df,\n",
        "        directory='data',\n",
        "        x_col=\"filename\",\n",
        "        y_col=\"class\",\n",
        "        target_size=shape[:2],\n",
        "        batch_size=10,\n",
        "        validate_filenames=False,\n",
        "        class_mode='categorical',color_mode=\"grayscale\",shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2113 non-validated image filenames belonging to 2 classes.\n",
            "Found 100 non-validated image filenames belonging to 2 classes.\n",
            "Found 782 non-validated image filenames belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhGTOcC-QZ1o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15f40368-a70f-467f-ea38-3188590dd42c"
      },
      "source": [
        "k.clear_session() #Clear keras backend \n",
        "try:\n",
        "  os.mkdir('models') #create folder for saving the trained networks\n",
        "except:\n",
        "  pass\n",
        "full_name='ResNet50V2-FPN-fold{}'.format(fold_num)\n",
        "classes_number=2 #Number of classes (normal and COVID-19)\n",
        "input_tensor=Input(shape=shape)\n",
        "weight_model = ResNet50V2(weights='imagenet', include_top=False) #Load ResNet50V2 ImageNet pre-trained weights\n",
        "weight_model.save_weights('weights.h5') #Save the weights\n",
        "base_model = ResNet50V2(weights=None, include_top=False, input_tensor=input_tensor) #Load the ResNet50V2 model without weights\n",
        "base_model.load_weights('weights.h5',skip_mismatch=True, by_name=True) #Load the ImageNet weights on the ResNet50V2 model except the first layer(because the first layer has one channel in our case)\n",
        "\n",
        "#Create Feature Pyramid Network (FPN)\n",
        "# We used some help for writing the Pyramid from the written code on https://github.com/fizyr/keras-retinanet/blob/master/keras_retinanet/models/retinanet.py\n",
        "feature_size=256 #Set the feature channels of the FPN\n",
        "layer_names = [\"conv4_block1_preact_relu\", \"conv5_block1_preact_relu\", \"post_relu\"] #Layers of ResNet50V2 with different scale features \n",
        "layer_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
        "C3, C4, C5=layer_outputs #Features of different scales, extracted from ResNet50V2\n",
        "P5           = keras.layers.Conv2D(feature_size, kernel_size=1, strides=1, padding='same', name='C5_reduced')(C5)\n",
        "P5_upsampled = layers.UpsampleLike(name='P5_upsampled')([P5, C4])\n",
        "P5           = keras.layers.Conv2D(feature_size, kernel_size=3, strides=1, padding='same', name='P5')(P5)\n",
        "\n",
        "# Concatenate P5 elementwise to C4\n",
        "P4           = keras.layers.Conv2D(feature_size, kernel_size=1, strides=1, padding='same', name='C4_reduced')(C4)\n",
        "P4           = keras.layers.Concatenate(axis=3)([P5_upsampled, P4])\n",
        "P4_upsampled = layers.UpsampleLike(name='P4_upsampled')([P4, C3])\n",
        "P4           = keras.layers.Conv2D(feature_size, kernel_size=3, strides=1, name='P4')(P4)\n",
        "\n",
        "# Concatenate P4 elementwise to C3\n",
        "P3 = keras.layers.Conv2D(feature_size, kernel_size=1, strides=1, padding='same', name='C3_reduced')(C3)\n",
        "P3 = keras.layers.Concatenate(axis=3)([P4_upsampled, P3])\n",
        "P3 = keras.layers.Conv2D(feature_size, kernel_size=3, strides=1, name='P3')(P3)\n",
        "\n",
        "# \"P6 is obtained via a 3x3 stride-2 conv on C5\"\n",
        "P6 = keras.layers.Conv2D(feature_size, kernel_size=3, strides=2, padding='same', name='P6')(C5)\n",
        "\n",
        "# \"P7 is computed by applying ReLU followed by a 3x3 stride-2 conv on P6\"\n",
        "P7 = keras.layers.Activation('relu', name='C6_relu')(P6)\n",
        "P7 = keras.layers.Conv2D(feature_size, kernel_size=3, strides=2, padding='same', name='P7')(P7)\n",
        "\n",
        "# Run classification for each of the generated features from the pyramid\n",
        "feature1 = Flatten()(P3)\n",
        "dp1 = Dropout(0.5)(feature1)\n",
        "preds1 = Dense(2, activation='relu',kernel_initializer=RandomNormal(mean=0.0, stddev=0.001))(dp1)\n",
        "#################################################################\n",
        "feature2 = Flatten()(P4)\n",
        "dp2 = Dropout(0.5)(feature2)\n",
        "preds2 = Dense(2, activation='relu',kernel_initializer=RandomNormal(mean=0.0, stddev=0.001))(dp2)\n",
        "#################################################################\n",
        "feature3 = Flatten()(P5)\n",
        "dp3= Dropout(0.5)(feature3)\n",
        "preds3 = Dense(2, activation='relu',kernel_initializer=RandomNormal(mean=0.0, stddev=0.001))(dp3)\n",
        "#################################################################\n",
        "feature4 = Flatten()(P6)\n",
        "dp4 = Dropout(0.5)(feature4)\n",
        "preds4 = Dense(2, activation='relu',kernel_initializer=RandomNormal(mean=0.0, stddev=0.001))(dp4)\n",
        "#################################################################\n",
        "feature5 = Flatten()(P7)\n",
        "dp5 = Dropout(0.5)(feature5)\n",
        "preds5 = Dense(2, activation='relu',kernel_initializer=RandomNormal(mean=0.0, stddev=0.001))(dp5)\n",
        "#################################################################\n",
        "concat=keras.layers.Concatenate(axis=1)([preds1,preds2,preds3,preds4,preds5]) #Concatenate the predictions(Classification results) of each of the pyramid features \n",
        "out=keras.layers.Dense(2,activation='softmax',kernel_initializer=RandomNormal(mean=0.0, stddev=0.001))(concat) #Final Classification\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=out) #Create the Training Model\n",
        "#######################################################\n",
        "for layer in model.layers:\n",
        "  layer.trainable = True\n",
        "model.compile(optimizer=optimizers.Nadam(lr=0.0001), loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "filepath=\"models/%s-{epoch:02d}-{val_accuracy:.4f}.hdf5\"%full_name  # Path to save the trained models\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', save_best_only=True, mode='max') #creating checkpoint to save the best validation accuracy\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94674944/94668760 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1316: UserWarning: Skipping loading of weights for layer conv1_conv due to mismatch in shape ((7, 7, 1, 64) vs (64, 3, 7, 7)).\n",
            "  weight_values[i].shape))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyTwxkkMxkN_",
        "outputId": "84d5d216-923e-45d3-c1e3-0123da4bae63"
      },
      "source": [
        "\r\n",
        "model.fit_generator(train_generator,epochs=20,validation_data=validation_generator,shuffle=True,callbacks=callbacks_list) #start training"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "151/151 [==============================] - 214s 1s/step - loss: 0.4557 - accuracy: 0.7889 - val_loss: 1.2924 - val_accuracy: 0.8400\n",
            "Epoch 2/20\n",
            "151/151 [==============================] - 174s 1s/step - loss: 0.2286 - accuracy: 0.9229 - val_loss: 0.4023 - val_accuracy: 0.9200\n",
            "Epoch 3/20\n",
            "151/151 [==============================] - 175s 1s/step - loss: 0.1764 - accuracy: 0.9371 - val_loss: 0.4336 - val_accuracy: 0.9200\n",
            "Epoch 4/20\n",
            "151/151 [==============================] - 174s 1s/step - loss: 0.1598 - accuracy: 0.9484 - val_loss: 0.0164 - val_accuracy: 0.9900\n",
            "Epoch 5/20\n",
            "151/151 [==============================] - 174s 1s/step - loss: 0.1405 - accuracy: 0.9508 - val_loss: 0.1317 - val_accuracy: 0.9200\n",
            "Epoch 6/20\n",
            "151/151 [==============================] - 174s 1s/step - loss: 0.1328 - accuracy: 0.9617 - val_loss: 0.7857 - val_accuracy: 0.9600\n",
            "Epoch 7/20\n",
            "151/151 [==============================] - 174s 1s/step - loss: 0.1104 - accuracy: 0.9607 - val_loss: 1.7192e-04 - val_accuracy: 0.9500\n",
            "Epoch 8/20\n",
            "151/151 [==============================] - 174s 1s/step - loss: 0.1121 - accuracy: 0.9617 - val_loss: 0.4536 - val_accuracy: 0.9800\n",
            "Epoch 9/20\n",
            "151/151 [==============================] - 174s 1s/step - loss: 0.0923 - accuracy: 0.9655 - val_loss: 4.9412e-04 - val_accuracy: 0.9800\n",
            "Epoch 10/20\n",
            "151/151 [==============================] - 174s 1s/step - loss: 0.1077 - accuracy: 0.9593 - val_loss: 0.3840 - val_accuracy: 0.9500\n",
            "Epoch 11/20\n",
            "151/151 [==============================] - 174s 1s/step - loss: 0.1018 - accuracy: 0.9621 - val_loss: 0.0112 - val_accuracy: 0.9800\n",
            "Epoch 12/20\n",
            "151/151 [==============================] - 174s 1s/step - loss: 0.0848 - accuracy: 0.9721 - val_loss: 0.0066 - val_accuracy: 0.9900\n",
            "Epoch 13/20\n",
            "151/151 [==============================] - 174s 1s/step - loss: 0.0863 - accuracy: 0.9726 - val_loss: 0.0277 - val_accuracy: 0.9800\n",
            "Epoch 14/20\n",
            "151/151 [==============================] - 175s 1s/step - loss: 0.0671 - accuracy: 0.9782 - val_loss: 0.0876 - val_accuracy: 0.9100\n",
            "Epoch 15/20\n",
            "151/151 [==============================] - 174s 1s/step - loss: 0.0860 - accuracy: 0.9735 - val_loss: 0.1186 - val_accuracy: 0.9500\n",
            "Epoch 16/20\n",
            "151/151 [==============================] - 174s 1s/step - loss: 0.0572 - accuracy: 0.9806 - val_loss: 0.1062 - val_accuracy: 0.9600\n",
            "Epoch 17/20\n",
            "151/151 [==============================] - 174s 1s/step - loss: 0.0470 - accuracy: 0.9849 - val_loss: 0.0428 - val_accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "151/151 [==============================] - 175s 1s/step - loss: 0.0508 - accuracy: 0.9834 - val_loss: 0.5107 - val_accuracy: 0.9500\n",
            "Epoch 19/20\n",
            "151/151 [==============================] - 174s 1s/step - loss: 0.0453 - accuracy: 0.9834 - val_loss: 5.8133e-04 - val_accuracy: 0.9900\n",
            "Epoch 20/20\n",
            "151/151 [==============================] - 174s 1s/step - loss: 0.0644 - accuracy: 0.9711 - val_loss: 0.0674 - val_accuracy: 0.9900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fe7cadde748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xE1h0BqQZ7C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c681510-abda-4af4-8896-051a7561e408"
      },
      "source": [
        "#Model Evaluation\n",
        "trained_models=[]\n",
        "for r,d,f in os.walk('models'): #Take the path to the trained nets \n",
        "  for file in f:\n",
        "    if '.hdf5' in file:\n",
        "      trained_models.append(os.path.join(r,file))\n",
        "\n",
        "reports=[]\n",
        "for trn_model in trained_models: #evaluate the network on each trained net\n",
        "  k.clear_session()\n",
        "  custom_object={'UpsampleLike': keras_retinanet.layers._misc.UpsampleLike}\n",
        "  netpath=trn_model \n",
        "  model_name=trn_model\n",
        "  fold_num=trn_model[trn_model.index('fold')+4] #find the fold number\n",
        "  net=keras.models.load_model(netpath, custom_objects=custom_object) #load model\n",
        "\n",
        "  covid_label= test_generator.class_indices['covid'] #get the index of COVID-19 class \n",
        "  normal_label= test_generator.class_indices['normal']  #get the index of normal class \n",
        "  tp=0 #True Positives\n",
        "  fp=0 #False Positives\n",
        "  anum=0 #All the images numbers\n",
        "  ###########\n",
        "  ctp=0 #Correct classified COVID-19 cases\n",
        "  cfp=0 #Wrong classified COVID-19 cases\n",
        "  cfn=0 #Not classified COVID-19 cases\n",
        "  ctn=0 #Correctly not classified COVID-19 cases\n",
        "  cnum=0 #Number of COVID cases\n",
        "  ################\n",
        "  ntp=0 #Correct classified normal cases\n",
        "  nfp=0 #Wrong classified normal cases\n",
        "  nfn=0 #Not classified normal cases\n",
        "  ntn=0 #Correctly not classified normal cases\n",
        "  nnum=0 #Number of normal cases\n",
        "  for num,img_name in enumerate(test_generator.filenames): #load image\n",
        "    gt_ind=test_generator.classes[num] #get the loaded image class index\n",
        "    img=cv2.imread(os.path.join('data',img_name),cv2.IMREAD_UNCHANGED) #load image\n",
        "    pred_ind=np.argmax(net.predict(np.expand_dims(np.expand_dims(img,axis=0),axis=3))[0]) #get the predicted class index\n",
        "    anum+=1 #count the number of images\n",
        "    if gt_ind==covid_label:\n",
        "      cnum+=1\n",
        "      if pred_ind==covid_label:\n",
        "        tp+=1\n",
        "        ctp+=1\n",
        "        ntn+=1\n",
        "      else:\n",
        "        fp+=1\n",
        "        nfp+=1\n",
        "        cfn+=1\n",
        "    elif gt_ind==normal_label:\n",
        "      nnum+=1\n",
        "      if pred_ind==normal_label:\n",
        "        ctn+=1\n",
        "        ntp+=1\n",
        "        tp+=1\n",
        "      else:\n",
        "        cfp+=1\n",
        "        nfn+=1\n",
        "        fp+=1\n",
        "\n",
        "  overall_acc=tp/(tp+fp) #overall accuracy\n",
        "  cacc=(ctp+ctn)/(ctp+ctn+cfp+cfn) #covid accurayc\n",
        "  nacc=(ntp+ntn)/(ntp+ntn+nfp+nfn) #normal accuracy\n",
        "  csens=ctp/(ctp+cfn) #covid sensitivity\n",
        "  nsens=ntp/(ntp+nfn) #normal sensitivity\n",
        "  cspec=ctn/(ctn+cfp) #covid specificity\n",
        "  nspec=ntn/(ntn+nfp) #normal specificity\n",
        "  cprec=ctp/(ctp+cfp) #covid precision\n",
        "  nprec=ntp/(ntp+nfp) #normal precision\n",
        "\n",
        "  reports.append([model_name,fold_num,tp,fp,ctp,cfn,cfp,ntp,nfn,nfp,overall_acc,cacc,nacc,csens,nsens,cspec,nspec,cprec,nprec])\n",
        "\n",
        "\n",
        "  print(model_name)\n",
        "  print('tp: ',tp,'fp: ',fp)\n",
        "\n",
        "with open('FPN.csv', mode='w',newline='') as csv_file:\n",
        "    csvwriter = csv.writer(csv_file, delimiter=',', quotechar='\"',quoting=csv.QUOTE_MINIMAL)\n",
        "    csvwriter.writerow(['model_name','fold_num','tp','fp','ctp','cfn','cfp','ntp','nfn','nfp','overall_acc','cacc','nacc','csens','nsens','cspec','nspec','cprec','nprec'])\n",
        "    for row in reports:\n",
        "        csvwriter.writerow(row)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "models/ResNet50V2-FPN-fold5-02-0.9200.hdf5\n",
            "tp:  721 fp:  61\n",
            "models/ResNet50V2-FPN-fold5-04-0.9900.hdf5\n",
            "tp:  763 fp:  19\n",
            "models/ResNet50V2-FPN-fold5-01-0.8400.hdf5\n",
            "tp:  627 fp:  155\n",
            "models/ResNet50V2-FPN-fold5-17-1.0000.hdf5\n",
            "tp:  778 fp:  4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-J5B4axhabZ"
      },
      "source": [
        "#Reference:\n",
        "#https://github.com/mr7495/COVID-CT-Code"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}